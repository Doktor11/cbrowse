TODO for final project
================================================================================

Immediately next: l. 281 in process.py - add sanity test

Process.py
--------------------------------------------------------------------------------

MAYBE	  - Improve the inconsistent URL finding algorithm specifically in the case
	    where a URL appears multiple times in a single result dictionary
MAYBE	  - Also consider just tabulating all URLs, not just inconsistent ones
DONE  	  - Do a top level reverse mapping from hash to all URLs that produce the 
	    hash; we're particularly interested in sets of URLs that produce the 
	    same hash because they automatically have unnecessary information
MAYBE	  - Possibly examine ALL URLs for potentially redundant information
	    - Harder to isolate unnecessary parts of URL if the unnecessary parts
	      are consistent across all calls, but this should be part of the 
	      analysis nevertheless
	  - Might want some way to map each hash to a unique, smaller number
	    so it's easier to detect hash equivalence by eye
DONE      - Take reduced URLs, pipe them to slimer and hash returns
DONE        - For each reduced URL, determine whether or not it was successful in 
	      returning the same contents
	    - Find a way to get the original results with some reduction
	    - Add a basic sanity test that re-fetches one of the URLs in the syn-
	      onym set and hashes its contents and compares that to the stored
	      hash



Urltable.py
--------------------------------------------------------------------------------
MAYBE	  - Looks like there can also be queries or params in other parts of the
	    URL... So have to do splitting within each 
IN PROG	  - Perhaps allow a customizable "similarity threshold", meaning a number
	    of differences to tolerate before considreing something a separate URL
	    or could there be a way of determining this dynamically?
DONE	    - Need to improve the similarity threshold; some URLs have a ton of params
	      that are different but are fundamentally the same but can't lower sim
	      threshold universally; perhaps weight different segments higher?
DONE	    - For example, if all netloc segments and path same, URLs very likely 
	      should be considered similar
NOT DOING - There is additional splitting that should be done; examples:
	    - amazon.com, https://pixel.adsafeprotected.com... commas within query
	      javascript? param is called "jsinfo"
	    - At the same time, there are sometimes unique comma-separated lists of 
	      numbers as param values; necessarily want to split those
IN PROG	  - Right now urls of different lengths are automatically not similar; may-
	      be this shouldn't be the case?
	    - See ebay.com-detailed.txt
	    - http://srx.main.ebayrtm.com/rtm... one has an extra parameter
	    - Maybe tie into weighting certain parts of URL more highly? netloc?
	    - If the names of all the parameters are the same but one URL has one
	      more, those actually might be good candidates for similarity
NOT DOING - Want some way to associate a hash with each inconsistent URL such that
	    - If a group of similar URLs all share a hash, a new URL can be const-
	      tructed that eliminates their unnecessary uniqueifying parts
DONE	  - Need to fix intersect URLs so that it can handle URLs of different lens
	    and potentially somewhat different #s of segments
	    - Consider: extra segment in the middle
	    - Extra segment at the end
	    - Equal # of segments, but different types
	    - Ended up just accepting that a set of synonym URLs might not be red-
	      ucible to a single reduced URL if the structure of the synonym URLs
	      isn't sufficiently similar
	  - Improve intersect URL to maintain as much non-unique parts of URLs as
	    possible
	    - Some param values are long encoded strings with only a small difference
	      in the whole string
	    - See huffingtonpost, "syndication.twitter.com"
	  - Improve URL insertion to maintain a separate similarity set to correspond
	    to each resource
	    - This means that 

	      

Urltrie.py
--------------------------------------------------------------------------------


survey.js (fetchsyn.js)
--------------------------------------------------------------------------------
DONE	  - Need to pipe simplified URLs from process.py back to this
DONE	  - Probably a lighter-weight version of this though; just want the hash
	    of their contents
	  - Currently NONE of the fetches are returning the same contents.
	    - Need to figure out why:
	    - URL reduction not working well?
	    - Some other error with script?
	    - Appears in some case that the existence of a seemingly meaningless
	      parameter matters, even if its value doesn't; ie. a new value won't
	      be automatically generated by the website
	    - in the case of this widget I get an "access denied" message


================================================================================
Final product spec
================================================================================
	- Possibly taking consistent parts of returned webpages and somehow
	  reconstructing the sites returned by Slimer and archiving them?
	- Removing or somehow standardizing the inconistent elements before arc-
	  hiving or at least modifying the elements in some way to show that
	  they might not be part of the "common browsing experience"
	- Currently displaying the plain html page displays something that is 
	  barely anything like the original page in most cases
	- How can I synthesize something that looks like the actual original
	  page with all the inconsistent elements removed or somehow marked?
	  Do I have to somehow automatically save all the relevant media files?
	

URLs for illustration:
================================================================================
	  
Reduced:
http://platform.twitter.com/widgets/follow_button.5f46501ecfda1c3e1c05dd3e24875611.en.html/#_=&dnt=false&id=twitter-widget-0&lang=en&screen_name=imgur&show_count=false&show_screen_name=false&size=m

2 Reducees:
http://platform.twitter.com/widgets/follow_button.5f46501ecfda1c3e1c05dd3e24875611.en.html#_=1421944306177&dnt=false&id=twitter-widget-0&lang=en&screen_name=imgur&show_count=false&show_screen_name=false&size=m
http://platform.twitter.com/widgets/follow_button.5f46501ecfda1c3e1c05dd3e24875611.en.html#_=1421944306280&dnt=false&id=twitter-widget-0&lang=en&screen_name=imgur&show_count=false&show_screen_name=false&size=m



