TODO for final project
================================================================================

first thing todo next: associate a hash with each URL given to urltable
      This won't work for URLs that map to multiple hashes; for inconsistent res
      ources, instead of a single hash, maybe pass some indicator that the URL
      is associated with multiple hashes and therefore isn't a good candidate
      for URL synonyms

Process.py
--------------------------------------------------------------------------------

	  - Improve the inconsistent URL finding algorithm specifically in the case
	    where a URL appears multiple times in a single result dictionary
	  - Also consider just tabulating all URLs, not just inconsistent ones
  	  - Do a top level reverse mapping from hash to all URLs that produce the 
	    hash; we're particularly interested in sets of URLs that produce the 
	    same hash because they automatically have unnecessary information
	  - Possibly examine ALL URLs for potentially redundant information
	    - Harder to isolate unnecessary parts of URL if the unnecessary parts
	      are consistent across all calls, but this should be part of the 
	      analysis nevertheless
	  - Might want some way to map each hash to a unique, smaller number
	    so it's easier to detect hash equivalence by eye

Urltable.py
--------------------------------------------------------------------------------
	  - Looks like there can also be queries or params in other parts of the
	    URL... So have to do splitting within each section
	  - Perhaps allow a customizable "similarity threshold", meaning a number
	    of differences to tolerate before considreing something a separate URL
	    or could there be a way of determining this dynamically?
	    - Need to improve the similarity threshold; some URLs have a ton of params
	      that are different but are fundamentally the same but can't lower sim
	      threshold universally; perhaps weight different segments higher?
	    - For example, if all netloc segments and path same, URLs very likely 
	      should be considered similar
	  - There is additional splitting that should be done; examples:
	    - amazon.com, https://pixel.adsafeprotected.com... commas within query
	      javascript? param is called "jsinfo"
	    - At the same time, there are sometimes unique comma-separated lists of 
	      numbers as param values; necessarily want to split those
	  - Right now urls of different lengths are automatically not similar; may-
	      be this shouldn't be the case?
	    - See ebay.com-detailed.txt
	    - http://srx.main.ebayrtm.com/rtm... one has an extra parameter
	    - Maybe tie into weighting certain parts of URL more highly? netloc?
	    - If the names of all the parameters are the same but one URL has one
	      more, those actually might be good candidates for similarity
          - Want some way to associate a hash with each inconsistent URL such that
	    - If a group of similar URLs all share a hash, a new URL can be const-
	      tructed that eliminates their unnecessary uniqueifying parts

Urltrie.py
--------------------------------------------------------------------------------




================================================================================
Final product spec
================================================================================
	- Possibly taking consistent parts of returned webpages and somehow
	  reconstructing the sites returned by Slimer and archiving them?
	- Removing or somehow standardizing the inconistent elements before arc-
	  hiving or at least modifying the elements in some way to show that
	  they might not be part of the "common browsing experience"
	- Currently displaying the plain html page displays something that is 
	  barely anything like the original page in most cases
	- How can I synthesize something that looks like the actual original
	  page with all the inconsistent elements removed or somehow marked?
	  Do I have to somehow automatically save all the relevant media files?
	
	  
